[{"path":"https://mlr3benchmark.mlr-org.com/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sonabend Raphael. Maintainer, author. Florian Pfisterer. Author. Michel Lang. Contributor. Bernd Bischl. Contributor.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Raphael S, Pfisterer F (2022). mlr3benchmark: Analysis Visualisation Benchmark Experiments. https://mlr3benchmark.mlr-org.com, https://github.com/mlr-org/mlr3benchmark.","code":"@Manual{,   title = {mlr3benchmark: Analysis and Visualisation of Benchmark Experiments},   author = {Sonabend Raphael and Florian Pfisterer},   year = {2022},   note = {https://mlr3benchmark.mlr-org.com, https://github.com/mlr-org/mlr3benchmark}, }"},{"path":"https://mlr3benchmark.mlr-org.com/index.html","id":"mlr3benchmark","dir":"","previous_headings":"","what":"Analysis and Visualisation of Benchmark Experiments","title":"Analysis and Visualisation of Benchmark Experiments","text":"Analysis tools benchmarking mlr3.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/index.html","id":"what-is-mlr3benchmark","dir":"","previous_headings":"","what":"What is mlr3benchmark?","title":"Analysis and Visualisation of Benchmark Experiments","text":"large benchmark experiment many tasks, learners, measures, don’t know begin analysis? want perform complete quantitative analysis benchmark results determine learner truly ‘best’? want visualise complex results benchmark experiments one line code? mlr3benchmark answer, least ’s finished maturing. mlr3benchmark enables fast efficient analysis benchmark experiments just lines code. long can coerce results format fitting classes (requirements), can perform benchmark analysis mlr3benchmark.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Analysis and Visualisation of Benchmark Experiments","text":"Install last release CRAN: Install development version GitHub:","code":"install.packages(\"mlr3benchmark\") remotes::install_github(\"mlr-org/mlr3benchmark\")"},{"path":"https://mlr3benchmark.mlr-org.com/index.html","id":"feature-overview","dir":"","previous_headings":"","what":"Feature Overview","title":"Analysis and Visualisation of Benchmark Experiments","text":"Currently mlr3benchmark supports analysis multiple learners multiple tasks. current implemented features best demonstrated example! First run mlr3 benchmark experiment: Now create BenchmarkAggr object analysis, objects store measure results aggregated resamplings: Now can begin analysis! mlr3benchmark, analysis multiple learners multiple independent tasks follows guidelines Demsar (2006). begin checking global Friedman test significant: significant difference rankings learners tasks? measures significant, now can proceed post-hoc tests. Now comparing learner post-hoc Friedman-Nemenyi tests: results tell us xgboost significantly different featureless model, comparisons non-significant. doesn’t tell us xgboost featureless better though, detailed information given critical difference diagram, note include minimize = FALSE accuracy maximised:  read diagram left right, learners left highest rank best performing, decrease going right. thick horizontal lines connect learners significantly difference ranked performance, tells us: xgboost significantly better featureless xgboost significantly better rpart rpart significantly better featureless Now visualise two much simpler plots display similar information, first mean standard error results across tasks, second boxplot across tasks:   conclude xgboost significantly better baseline significantly better decision tree decision tree significantly better baseline, recommend xgboost now. analysis complete!","code":"library(mlr3) library(mlr3learners) library(ggplot2) set.seed(1)  task = tsks(c(\"iris\", \"sonar\", \"wine\", \"zoo\")) learns = lrns(c(\"classif.featureless\", \"classif.rpart\", \"classif.xgboost\")) bm = benchmark(benchmark_grid(task, learns, rsmp(\"cv\", folds = 3))) # these measures are the same but we'll continue for the example ba = as.BenchmarkAggr(bm, measures = msrs(c(\"classif.acc\", \"classif.ce\"))) ba ## <BenchmarkAggr> of 12 rows with 4 tasks, 3 learners and 2 measures ##     task_id  learner_id       acc         ce ##  1:    iris featureless 0.2800000 0.72000000 ##  2:    iris       rpart 0.9466667 0.05333333 ##  3:    iris     xgboost 0.9600000 0.04000000 ##  4:   sonar featureless 0.5334023 0.46659765 ##  5:   sonar       rpart 0.6537612 0.34623879 ##  6:   sonar     xgboost 0.6394755 0.36052450 ##  7:    wine featureless 0.3990584 0.60094162 ##  8:    wine       rpart 0.8652542 0.13474576 ##  9:    wine     xgboost 0.9048023 0.09519774 ## 10:     zoo featureless 0.4058229 0.59417706 ## 11:     zoo       rpart 0.8309566 0.16904337 ## 12:     zoo     xgboost 0.9099822 0.09001783 ba$friedman_test() ##      X2 df    p.value p.signif ## acc 6.5  2 0.03877421        * ## ce  6.5  2 0.03877421        * ba$friedman_posthoc(meas = \"acc\") ##  ##  Pairwise comparisons using Nemenyi multiple comparison test  ##              with q approximation for unreplicated blocked data  ##  ## data:  acc and learner_id and task_id  ##  ##         featureless rpart ## rpart   0.181       -     ## xgboost 0.036       0.759 ##  ## P value adjustment method: none autoplot(ba, type = \"cd\", meas = \"acc\", minimize = FALSE) autoplot(ba, meas = \"acc\") autoplot(ba, type = \"box\", meas = \"acc\")"},{"path":"https://mlr3benchmark.mlr-org.com/index.html","id":"roadmap","dir":"","previous_headings":"","what":"Roadmap","title":"Analysis and Visualisation of Benchmark Experiments","text":"mlr3benchmark early stages interface still maturing, near-future updates include: Extending BenchmarkAggr non-independent tasks Extending BenchmarkAggr single tasks Adding BenchmarkScore non-aggregated measures, e.g. observation-level scores Bayesian methods analysis","code":""},{"path":"https://mlr3benchmark.mlr-org.com/index.html","id":"bugs-questions-feedback","dir":"","previous_headings":"","what":"Bugs, Questions, Feedback","title":"Analysis and Visualisation of Benchmark Experiments","text":"mlr3benchmark free open source software project encourages participation feedback. issues, questions, suggestions feedback, please hesitate open “issue” GitHub page! case problems / bugs, often helpful provide “minimum working example” showcases behaviour (don’t worry bug obvious).","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":null,"dir":"Reference","previous_headings":"","what":"Aggregated Benchmark Result Object — BenchmarkAggr","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"R6 class aggregated benchmark results.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"class used easily carry guide analysis models aggregating results resampling. can either constructed using mlr3 objects, example result mlr3::BenchmarkResult$aggregate via .BenchmarkAggr, passing custom dataset results. Custom datasets must include least, character column learner ids, character column task ids, numeric columns one measures. Currently supported multiple independent datasets .","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"`r format_bib(\"demsar_2006\")","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"data (data.table::data.table)  Aggregated data. learners (character())  Unique learner names. tasks (character())  Unique task names. measures (character())  Unique measure names. nlrns (integer())  Number learners. ntasks (integer())  Number tasks. nmeas (integer())  Number measures. nrow (integer())  Number rows. col_roles (character())  Column roles, currently changed construction.","code":""},{"path":[]},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"BenchmarkAggr$new() BenchmarkAggr$print() BenchmarkAggr$summary() BenchmarkAggr$rank_data() BenchmarkAggr$friedman_test() BenchmarkAggr$friedman_posthoc() BenchmarkAggr$subset() BenchmarkAggr$clone()","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$new(   dt,   task_id = \"task_id\",   learner_id = \"learner_id\",   independent = TRUE,   strip_prefix = TRUE,   ... )"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"dt (matrix(1)) ' matrix like object coercable data.table::data.table, include column names \"task_id\" \"learner_id\", least one measure (numeric). ids already factors coerced internally. task_id (character(1))  String specifying name task id column. learner_id (character(1)) String specifying name learner id column. independent (logical(1))  tasks independent one another? Affects tests can used analysis. strip_prefix (logical(1))  TRUE (default) mlr prefixes, e.g. regr., classif., automatically stripped learner_id. ...  Additional arguments, currently unused.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"Prints internal data via data.table::print.data.table.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$print(...)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"...  Passed data.table::print.data.table.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-summary-","dir":"Reference","previous_headings":"","what":"Method summary()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"Prints internal data via data.table::print.data.table.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$summary(...)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"...  Passed data.table::print.data.table.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-rank-data-","dir":"Reference","previous_headings":"","what":"Method rank_data()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"Ranks aggregated data given measure.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$rank_data(meas = NULL, minimize = TRUE, task = NULL, ...)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"meas (character(1))  Measure rank data , $measures. Can NULL one measure data. minimize (logical(1))  measure minimized? Default TRUE. task (character(1))  NULL returns matrix ranks columns tasks rows learners, otherwise returns one-column matrix specified task, $tasks. ...  Passed data.table::frank().","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-friedman-test-","dir":"Reference","previous_headings":"","what":"Method friedman_test()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"Computes Friedman test tasks, assumes datasets independent.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$friedman_test(meas = NULL, p.adjust.method = NULL)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"meas (character(1))  Measure rank data , $measures. measure provided returns matrix tests measures. p.adjust.method (character(1))  Passed p.adjust meas = NULL multiple testing correction. NULL correction applied.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-friedman-posthoc-","dir":"Reference","previous_headings":"","what":"Method friedman_posthoc()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"Posthoc Friedman Nemenyi tests. Computed PMCMRplus::frdAllPairsNemenyiTest. global $friedman_test non-significant returned post-hocs computed. Also returns critical difference","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$friedman_posthoc(meas = NULL, p.value = 0.05)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"meas (character(1))  Measure rank data , $measures. Can NULL one measure data. p.value (numeric(1))  p.value global test considered significant.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-subset-","dir":"Reference","previous_headings":"","what":"Method subset()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"Subsets data given tasks learners. Returns data data.table::data.table.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$subset(task = NULL, learner = NULL)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"task (character())  Task(s) subset data . learner (character())  Learner(s) subset data .","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"objects class cloneable method.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"BenchmarkAggr$clone(deep = FALSE)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/BenchmarkAggr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Aggregated Benchmark Result Object — BenchmarkAggr","text":"","code":"# Not restricted to mlr3 objects df = data.frame(tasks = factor(rep(c(\"A\", \"B\"), each = 5),                                levels = c(\"A\", \"B\")),                 learners = factor(paste0(\"L\", 1:5)),                 RMSE = runif(10), MAE = runif(10)) as.BenchmarkAggr(df, task_id = \"tasks\", learner_id = \"learners\") #> <BenchmarkAggr> of 10 rows with 2 tasks, 5 learners and 2 measures #>     tasks learners      RMSE        MAE #>  1:     A       L1 0.8091006 0.02323056 #>  2:     A       L2 0.1049172 0.91925909 #>  3:     A       L3 0.4998557 0.25892239 #>  4:     A       L4 0.7777033 0.36519280 #>  5:     A       L5 0.1314443 0.54306437 #>  6:     B       L1 0.3842120 0.44454268 #>  7:     B       L2 0.4498474 0.65444683 #>  8:     B       L3 0.7126114 0.90544788 #>  9:     B       L4 0.6266076 0.48093577 #> 10:     B       L5 0.4974439 0.75951330  if (requireNamespaces(c(\"mlr3\", \"rpart\"))) {   library(mlr3)   task = tsks(c(\"boston_housing\", \"mtcars\"))   learns = lrns(c(\"regr.featureless\", \"regr.rpart\"))   bm = benchmark(benchmark_grid(task, learns, rsmp(\"cv\", folds = 2)))    # coercion   as.BenchmarkAggr(bm) } #> <BenchmarkAggr> of 4 rows with 2 tasks, 2 learners and 1 measure #>           task_id  learner_id       mse #> 1: boston_housing featureless 85.024391 #> 2: boston_housing       rpart  3.689093 #> 3:         mtcars featureless 43.141348 #> 4:         mtcars       rpart 43.141348"},{"path":"https://mlr3benchmark.mlr-org.com/reference/as.BenchmarkAggr.html","id":null,"dir":"Reference","previous_headings":"","what":"Coercions to BenchmarkAggr — as.BenchmarkAggr","title":"Coercions to BenchmarkAggr — as.BenchmarkAggr","text":"Coercion methods BenchmarkAggr. mlr3::BenchmarkResult simple wrapper around BenchmarkAggr constructor called mlr3::BenchmarkResult$aggregate().","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/as.BenchmarkAggr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coercions to BenchmarkAggr — as.BenchmarkAggr","text":"","code":"as.BenchmarkAggr(   obj,   task_id = \"task_id\",   learner_id = \"learner_id\",   independent = TRUE,   strip_prefix = TRUE,   ... )"},{"path":"https://mlr3benchmark.mlr-org.com/reference/as.BenchmarkAggr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coercions to BenchmarkAggr — as.BenchmarkAggr","text":"obj (mlr3::BenchmarkResult|matrix(1))  Passed BenchmarkAggr$new(). task_id, learner_id, independent, strip_prefix See BenchmarkAggr$initialize(). ...  Passed mlr3::BenchmarkResult$aggregate().","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/as.BenchmarkAggr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coercions to BenchmarkAggr — as.BenchmarkAggr","text":"","code":"df = data.frame(tasks = factor(rep(c(\"A\", \"B\"), each = 5),                                levels = c(\"A\", \"B\")),                 learners = factor(paste0(\"L\", 1:5)),                 RMSE = runif(10), MAE = runif(10))  as.BenchmarkAggr(df, task_id = \"tasks\", learner_id = \"learners\") #> <BenchmarkAggr> of 10 rows with 2 tasks, 5 learners and 2 measures #>     tasks learners      RMSE        MAE #>  1:     A       L1 0.3293004 0.02540927 #>  2:     A       L2 0.4439394 0.78438239 #>  3:     A       L3 0.6421501 0.85532471 #>  4:     A       L4 0.9973985 0.40990012 #>  5:     A       L5 0.9537036 0.50983412 #>  6:     B       L1 0.5157478 0.75447485 #>  7:     B       L2 0.7193196 0.04474314 #>  8:     B       L3 0.4448212 0.67339099 #>  9:     B       L4 0.1448374 0.23954644 #> 10:     B       L5 0.4264680 0.02603675   if (requireNamespaces(c(\"mlr3\", \"rpart\"))) {   library(mlr3)   task = tsks(c(\"boston_housing\", \"mtcars\"))   learns = lrns(c(\"regr.featureless\", \"regr.rpart\"))   bm = benchmark(benchmark_grid(task, learns, rsmp(\"cv\", folds = 2)))    # default measure   as.BenchmarkAggr(bm)    # change measure   as.BenchmarkAggr(bm, measures = msr(\"regr.rmse\")) } #> <BenchmarkAggr> of 4 rows with 2 tasks, 2 learners and 1 measure #>           task_id  learner_id     rmse #> 1: boston_housing featureless 9.201929 #> 2: boston_housing       rpart 1.745418 #> 3:         mtcars featureless 6.898683 #> 4:         mtcars       rpart 6.898683"},{"path":"https://mlr3benchmark.mlr-org.com/reference/autoplot.BenchmarkAggr.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots for BenchmarkAggr — autoplot.BenchmarkAggr","title":"Plots for BenchmarkAggr — autoplot.BenchmarkAggr","text":"Generates plots BenchmarkAggr, assume multiple, independent, tasks. Choices depending argument type: \"mean\" (default): Assumes least two independent tasks. Plots sample mean measure learners error bars computed standard error mean. \"box\": Boxplots learner calculated tasks given measure. \"fn\": Plots post-hoc Friedman-Nemenyi first calling BenchmarkAggr$friedman_posthoc plotting significant pairs coloured squares leaving non-significant pairs blank, useful simply visualising pair-wise comparisons. \"cd\": Critical difference plots (Demsar, 2006). Learners drawn x-axis according average rank best performing left decreasing performance going right. learners connected horizontal bar significantly different performance. Critical differences calculated : $$CD = q_{\\alpha} \\sqrt{\\left(\\frac{k(k+1)}{6N}\\right)}$$ \\(q_\\alpha\\) based studentized range statistic. See references details. recommended crop white space using external tools, function image_trim() package magick.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/autoplot.BenchmarkAggr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots for BenchmarkAggr — autoplot.BenchmarkAggr","text":"","code":"# S3 method for BenchmarkAggr autoplot(   obj,   type = c(\"mean\", \"box\", \"fn\", \"cd\"),   meas = NULL,   level = 0.95,   p.value = 0.05,   minimize = TRUE,   test = \"nem\",   baseline = NULL,   style = 1L,   ratio = 1/7,   col = \"red\",   friedman_global = TRUE,   ... )"},{"path":"https://mlr3benchmark.mlr-org.com/reference/autoplot.BenchmarkAggr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots for BenchmarkAggr — autoplot.BenchmarkAggr","text":"obj BenchmarkAggr type (character(1))  Type plot, see description. meas (character(1))  Measure plot, obj$measures, can NULL one measure obj. level (numeric(1))  Confidence level error bars type = \"mean\" p.value (numeric(1))  value considered significant type = \"cd\" type = \"fn\". minimize (logical(1))  type = \"cd\", indicates measure optimally minimized. Default TRUE. test (character(1)))  type = \"cd\", critical differences either computed learners (test = \"nemenyi\"), baseline (test = \"bd\"). Bonferroni-Dunn usually yields higher power Nemenyi compares algorithms one baseline. Default \"nemenyi\". baseline (character(1))  type = \"cd\" test = \"bd\" baseline learner compare learners , $learners, NULL differences compared best performing learner. style (integer(1))  type = \"cd\" two ggplot styles shipped package (style = 1 style = 2), otherwise data can accessed via returned ggplot. ratio (numeric(1))  type = \"cd\" style = 1, passed ggplot2::coord_fixed(), useful quickly specifying aspect ratio plot, best used ggsave(). col (character(1)) type = \"fn\", specifies color fill significant tiles, default \"red\". friedman_global (logical(1)) friedman global test performed fortype = \"cd\" type = \"fn\"? FALSE, warning issued case corresponding friedman posthoc test fails instead error. Default TRUE (raises error). ...  Additional arguments, currently unused.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/autoplot.BenchmarkAggr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plots for BenchmarkAggr — autoplot.BenchmarkAggr","text":"Demšar J (2006). “Statistical Comparisons Classifiers Multiple Data Sets.” Journal Machine Learning Research, 7(1), 1-30. https://jmlr.org/papers/v7/demsar06a.html.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/autoplot.BenchmarkAggr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plots for BenchmarkAggr — autoplot.BenchmarkAggr","text":"","code":"if (requireNamespaces(c(\"mlr3learners\", \"mlr3\", \"rpart\", \"xgboost\"))) { library(mlr3) library(mlr3learners) library(ggplot2)  set.seed(1) task = tsks(c(\"iris\", \"sonar\", \"wine\", \"zoo\")) learns = lrns(c(\"classif.featureless\", \"classif.rpart\", \"classif.xgboost\")) bm = benchmark(benchmark_grid(task, learns, rsmp(\"cv\", folds = 3))) obj = as.BenchmarkAggr(bm)  # mean and error bars autoplot(obj, type = \"mean\", level = 0.95)  if (requireNamespace(\"PMCMRplus\", quietly = TRUE)) {   # critical differences   autoplot(obj, type = \"cd\",style = 1)   autoplot(obj, type = \"cd\",style = 2)    # post-hoc friedman-nemenyi   autoplot(obj, type = \"fn\") }  }"},{"path":"https://mlr3benchmark.mlr-org.com/reference/mlr3benchmark-package.html","id":null,"dir":"Reference","previous_headings":"","what":"mlr3benchmark: Analysis and Visualisation of Benchmark Experiments — mlr3benchmark-package","title":"mlr3benchmark: Analysis and Visualisation of Benchmark Experiments — mlr3benchmark-package","text":"Implements methods post-hoc analysis     visualisation benchmark experiments, 'mlr3' beyond.","code":""},{"path":[]},{"path":"https://mlr3benchmark.mlr-org.com/reference/mlr3benchmark-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mlr3benchmark: Analysis and Visualisation of Benchmark Experiments — mlr3benchmark-package","text":"Maintainer: Sonabend Raphael raphaelsonabend@gmail.com (ORCID) Authors: Florian Pfisterer pfistererf@googlemail.com (ORCID) contributors: Michel Lang michellang@gmail.com (ORCID) [contributor] Bernd Bischl bernd_bischl@gmx.net (ORCID) [contributor]","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/requireNamespaces.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper Vectorizing requireNamespace — requireNamespaces","title":"Helper Vectorizing requireNamespace — requireNamespaces","text":"Internal helper function documentation.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/reference/requireNamespaces.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper Vectorizing requireNamespace — requireNamespaces","text":"","code":"requireNamespaces(x)"},{"path":"https://mlr3benchmark.mlr-org.com/reference/requireNamespaces.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper Vectorizing requireNamespace — requireNamespaces","text":"x Packages check.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/news/index.html","id":"mlr3benchmark-014","dir":"Changelog","previous_headings":"","what":"mlr3benchmark 0.1.4","title":"mlr3benchmark 0.1.4","text":"Add friedman_global argument posthoc tests autoplots allow methods plots run even global Friedman test fails (.e. don’t reject null)","code":""},{"path":"https://mlr3benchmark.mlr-org.com/news/index.html","id":"mlr3benchmark-013","dir":"Changelog","previous_headings":"","what":"mlr3benchmark 0.1.3","title":"mlr3benchmark 0.1.3","text":"CRAN release: 2021-10-04 Fix README Fix PMCMRplus","code":""},{"path":"https://mlr3benchmark.mlr-org.com/news/index.html","id":"mlr3benchmark-012","dir":"Changelog","previous_headings":"","what":"mlr3benchmark 0.1.2","title":"mlr3benchmark 0.1.2","text":"CRAN release: 2021-04-19 Critical patch bug creating BenchmarkAggr objects. Task learner columns must now provided factors BenchmarkAggr objects, internal coercion made. Bug fix CD plots","code":""},{"path":"https://mlr3benchmark.mlr-org.com/news/index.html","id":"mlr3benchmark-011","dir":"Changelog","previous_headings":"","what":"mlr3benchmark 0.1.1","title":"mlr3benchmark 0.1.1","text":"CRAN release: 2020-12-16 BenchmarkAggr$friedman_test now returns full test object single measure exists object Fixed plotting autoplot.BenchmarkAggr CD-plots, previously bars overlapping giving misleading results. BenchmarkAggr now flexible construction. Instead forced name columns task_id learner_id, instead name can used passed respective arguments constructor. Adds $subset public method BenchmarkAggr thin wrapper around subset data.table. Returns subsetted data.table.","code":""},{"path":"https://mlr3benchmark.mlr-org.com/news/index.html","id":"mlr3benchmark-010","dir":"Changelog","previous_headings":"","what":"mlr3benchmark 0.1.0","title":"mlr3benchmark 0.1.0","text":"CRAN release: 2020-11-19 Initial CRAN release","code":""}]
